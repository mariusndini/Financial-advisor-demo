{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "816e91fd-d1c3-4087-883d-f7974c5b344e",
   "metadata": {
    "collapsed": false,
    "name": "Notebook_Docs"
   },
   "source": [
    "# Earnings Call Grading Notebook\n",
    "\n",
    "This notebook will take all the earnings calls that are saved in Snowflake daily, iterate thru each call and grade each call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "Setup"
   },
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from io import StringIO\n",
    "import re\n",
    "import math\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "# Import Snowpark session\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "\n",
    "# Initialize Snowpark session\n",
    "session = get_active_session()\n",
    "\n",
    "# Configuration parameters\n",
    "DATE_STR = str(date.today())\n",
    "DAYS_TO_ITERATE = 3\n",
    "SPLIT_TOKENS = 125000  # Max number of tokens to send to summary LLM Call\n",
    "AI_MODEL = 'llama3.1-405b'\n",
    "\n",
    "# Print the configuration parameters for verification\n",
    "print(f\"Configuration - Date: {DATE_STR}, Days to Iterate: {DAYS_TO_ITERATE}, Split Tokens: {SPLIT_TOKENS}, AI Model: {AI_MODEL}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cedd7ac-9b12-42d9-b6e6-92585189b0b4",
   "metadata": {
    "collapsed": false,
    "name": "cell1"
   },
   "source": [
    "## Call Check\n",
    "Check which alls are not graded and grade only the calls which are not yet graded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d07e6a-b14d-484a-b0ca-2ebf9201d331",
   "metadata": {
    "language": "sql",
    "name": "GetTranscriptsToProcess"
   },
   "outputs": [],
   "source": [
    "-- with T_MINUS as(\n",
    "--     select STOCK_SYMBOL, FISCAL_QUARTER::number as FQ, FISCAL_YEAR::Number AS FY\n",
    "--     from EARNINGS.PUBLIC.TRANSCRIPTS\n",
    "--     MINUS\n",
    "--     select STOCK_SYMBOL, FISCAL_QUARTER::Number, FISCAL_YEAR::Number\n",
    "--     from EARNINGS.ANALYSIS.EARNINGS_SUMMARIES\n",
    "-- )\n",
    "-- select TR.*\n",
    "-- from EARNINGS.PUBLIC.TRANSCRIPTS TR inner join T_MINUS T\n",
    "-- on T.STOCK_SYMBOL = TR.STOCK_SYMBOL\n",
    "-- AND T.FQ = TR.FISCAL_QUARTER \n",
    "-- AND T.FY = TR.FISCAL_YEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a7e321-5a1d-4cad-9cc3-1821eb3451c0",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": [
    "\n",
    "getSQL = f'''\n",
    "    with T_MINUS as(\n",
    "        select STOCK_SYMBOL, FISCAL_QUARTER::number as FQ, FISCAL_YEAR::Number AS FY\n",
    "        from EARNINGS.PUBLIC.TRANSCRIPTS\n",
    "        MINUS\n",
    "        select STOCK_SYMBOL, FISCAL_QUARTER::Number, FISCAL_YEAR::Number\n",
    "        from EARNINGS.ANALYSIS.EARNINGS_SUMMARIES\n",
    "    )\n",
    "    select TR.*\n",
    "    from EARNINGS.PUBLIC.TRANSCRIPTS TR inner join T_MINUS T\n",
    "    on T.STOCK_SYMBOL = TR.STOCK_SYMBOL\n",
    "    AND T.FQ = TR.FISCAL_QUARTER \n",
    "    AND T.FY = TR.FISCAL_YEAR\n",
    "    ORDER BY QUERY_DATE DESC\n",
    "'''\n",
    "\n",
    "TranscriptsToProcess = session.sql(getSQL)\n",
    "print(TranscriptsToProcess.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5d9989-9c10-4037-8fc7-6563f604ffd3",
   "metadata": {
    "collapsed": false,
    "name": "cell2"
   },
   "source": [
    "## Extract JSON Helper Function\n",
    "\n",
    "This function designed to sanitize and extract JSON data from a given input string, typically containing unstructured or semi-structured data that may include special characters and formatting issues.\n",
    "\n",
    "This function and others like it are critical for Gen-AI and LLM applications because their output may not be as expected.\n",
    "\n",
    "For example the LLM may believe it is speaking to a human and say something like:\n",
    "\n",
    "```\n",
    "Below is your JSON\n",
    "    {JSON here:[]}\n",
    "Let me know if you need anything else\n",
    "```\n",
    "\n",
    "The above may have valid JSON within it but the english header/footer are break the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee1467c-5677-4fad-8bd1-dca207da6aaf",
   "metadata": {
    "language": "python",
    "name": "ExtractJSONCell"
   },
   "outputs": [],
   "source": [
    "def extractJSON(result):\n",
    "    # Replace problematic characters\n",
    "    sanitized_result = re.sub(r'[\\n\\r\\u0085\\u2028\\u2029]+', ' ', str(result))\n",
    "    sanitized_result = re.sub(r'\" \"sections\"', '\", \"sections\"', sanitized_result)\n",
    "\n",
    "    # Extract JSON from the sanitized string\n",
    "    json_match = re.search(r'{.*}', sanitized_result, re.DOTALL)\n",
    "    \n",
    "    if json_match:\n",
    "        json_str = json_match.group()\n",
    "        try:\n",
    "            final_summary = json.loads(json_str)\n",
    "            return final_summary\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(\"Sanitized Result:\", sanitized_result)\n",
    "            print(f\"JSONDecodeError: {e}\")\n",
    "    else:\n",
    "        print(\"No valid JSON found in the result.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e9d5d3-7b1c-4a2a-a527-c0504cd0ad6e",
   "metadata": {
    "collapsed": false,
    "name": "cell3"
   },
   "source": [
    "## Splitter Function\n",
    "Depending on your the model the call log may need to be split into smaller chunks and send to the LLM function via many iterations. Different functions have different [context windows](https://docs.snowflake.com/en/user-guide/snowflake-cortex/llm-functions#model-restrictions). \n",
    "\n",
    "The **SPLIT_TOKENS = 30000** and **AI_MODEL = 'mistral-large'** are critical for the notebook and the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a71da46-178c-45ee-8f90-11d341dc56c5",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "splitFunction"
   },
   "outputs": [],
   "source": [
    "def split_into_chunks(text):\n",
    "    # Query to calculate the number of tokens required to split the text\n",
    "    query = f\"SELECT CEIL(SNOWFLAKE.CORTEX.COUNT_TOKENS( '{AI_MODEL}', '{text}' )/{SPLIT_TOKENS}) as TOKEN_LEN;\"\n",
    "    token_split_len = session.sql(query).to_pandas().iloc[0]['TOKEN_LEN']\n",
    "    \n",
    "    print('splitting - ', end=\" \")\n",
    "    \n",
    "    # Tokenize the text using regular expressions to capture words and punctuation\n",
    "    tokens = re.findall(r'\\S+|\\n', text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    # Calculate the maximum number of tokens per chunk\n",
    "    max_split_tokens = int(math.ceil(len(text) / token_split_len))\n",
    "    \n",
    "    print(f'({token_split_len}, {max_split_tokens}) -', end=\" \")\n",
    "    \n",
    "    for token in tokens:\n",
    "        token_length = len(token) + 1  # Adding 1 for the space or punctuation\n",
    "        \n",
    "        if current_length + token_length >= max_split_tokens:\n",
    "            # Join current_chunk into a string and append to chunks\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            # Start a new chunk\n",
    "            current_chunk = [token]\n",
    "            current_length = token_length\n",
    "        else:\n",
    "            # Add the token to the current chunk\n",
    "            current_chunk.append(token)\n",
    "            current_length += token_length\n",
    "    \n",
    "    # Append the last chunk if it has any tokens\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80810d26-d5f2-451d-bd6a-6d868f83a336",
   "metadata": {
    "collapsed": false,
    "name": "cell4"
   },
   "source": [
    "## Get Transcript\n",
    "Go to the transcript table and get the transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae2546c-a37c-4a0b-abd3-d7bd1c4f0fc3",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "GetTranscript"
   },
   "outputs": [],
   "source": [
    "def getTranscript(ticker, querydate):\n",
    "    print(f'{ticker} | Transcript - ', end=\" \")\n",
    "\n",
    "    SQLTxt = f\"\"\"\n",
    "        SELECT \n",
    "            STOCK_SYMBOL, \n",
    "            FISCAL_QUARTER, \n",
    "            FISCAL_YEAR, \n",
    "            TRANSCRIPT, \n",
    "            RELEASE_DATE\n",
    "        FROM \n",
    "            EARNINGS.public.transcripts\n",
    "        WHERE \n",
    "            stock_symbol = '{ticker}'\n",
    "            AND query_date = '{querydate}'\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        result = session.sql(SQLTxt).collect()\n",
    "        if result:\n",
    "            transcript = str(result[0]['TRANSCRIPT'])\n",
    "            fiscal_quarter = result[0]['FISCAL_QUARTER']\n",
    "            fiscal_year = result[0]['FISCAL_YEAR']\n",
    "            release_date = str(result[0]['RELEASE_DATE'])\n",
    "            return transcript, fiscal_quarter, fiscal_year, release_date, ''\n",
    "            \n",
    "        else:\n",
    "            return '', '', '', '', ''\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching transcript: {e}\")\n",
    "        return '', '', '', '', ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18978200-b344-4b42-8aeb-d40c2e42e141",
   "metadata": {
    "collapsed": false,
    "name": "cell5"
   },
   "source": [
    "# Summarize using Cortex Complete \n",
    "In this function we actually grade and pull out any relevent information from the financial call.\n",
    "\n",
    "In the prompt below we provide **instructions** for the LLM on what actions to perform.\n",
    "\n",
    "we also provide the **Segment** which is the call it self or portions of the call.\n",
    "\n",
    "The LLM is then asked to **output** any specific information we are asking for such as performance, initiatives, challenges and any annoucements from the call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd9f039-0a0d-4d6c-a0dd-0441b18bb0d4",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "SplitTranscript"
   },
   "outputs": [],
   "source": [
    "def splitTransacript(escaped_transcript):\n",
    "    chunks = split_into_chunks(escaped_transcript )\n",
    "    \n",
    "    # Summarize each chunk\n",
    "    results = []\n",
    "    for chunk in chunks:\n",
    "        MLTXT = f\"\"\"\n",
    "            SELECT SNOWFLAKE.CORTEX.COMPLETE(\n",
    "                '{AI_MODEL}',\n",
    "                ARRAY_CONSTRUCT(\n",
    "                    OBJECT_CONSTRUCT(\n",
    "                        'role', 'user',\n",
    "                        'content', '### INSTRUCTIONS\n",
    "                            You will be provided with a segment of a financial earnings call transcript. \n",
    "                            Your task is to summarize this segment accurately and concisely, focusing on key points such as financial performance, strategic initiatives, Challenges, significant announcements, and any other relevant information.\n",
    "                            Ensure the summary is clear and easy to understand for a broad audience, including investors, analysts, and business professionals.\n",
    "\n",
    "                            Be descriptive in your analysis, do not worry about providing short summary. \n",
    "                            \n",
    "                            <Segment>{chunk}</Segment>\n",
    "\n",
    "                            ### OUTPUT\n",
    "                            Provide a well-structured summary in the following format:\n",
    "                            0. Please make sure to include the company name.\n",
    "                            1. Financial Performance: ...\n",
    "                            2. Strategic Initiatives: ...\n",
    "                            3. Challenges: ...\n",
    "                            4. Significant Announcements: ...\n",
    "                            5. Other Relevant Information: ...\n",
    "\n",
    "                            Only include the summary and nothing else.\n",
    "                            \n",
    "                            ### EXCLUDE\n",
    "                            DO NOT any new lines or any characters that will invalidate JSON\n",
    "                            Do not mention or write COMPANY ABC or COMPANY XYZ use the company name.\n",
    "                        '\n",
    "                    )\n",
    "                ),\n",
    "                OBJECT_CONSTRUCT('temperature', 0.7)\n",
    "            ) AS result\n",
    "        \"\"\"\n",
    "        \n",
    "        result = session.sql(MLTXT).collect()\n",
    "        results.append(result[0]['RESULT'])\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30151d20-64d4-4085-9d70-63c7a26359a4",
   "metadata": {
    "collapsed": false,
    "name": "cell6"
   },
   "source": [
    "# Summarize and Grade \n",
    "\n",
    "Below we ask the LLM to take all the extracted information above and summarize it into a relevent cohesive JSON output.\n",
    "\n",
    "We ask the LLM to be a world-class financial report and provide us with the **title of the article**, an introduction, detailed analysis of the call and then any positives or negatives about the call. \n",
    "\n",
    "This condensed portion of the call will be used to display in a streamlit application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe0f3d0-119a-44db-a342-99d6fd2b1801",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "Summarize"
   },
   "outputs": [],
   "source": [
    "def summarizeTransacript(results):\n",
    "    print('summarizing - ', end =\" \")\n",
    "    # Convert the results to JSON\n",
    "    results_json = [json.loads(result) for result in results]\n",
    "\n",
    "    # Extract just the messages from the results\n",
    "    extracted_messages = ' '.join([res['choices'][0]['messages'] for res in results_json])\n",
    "    \n",
    "    ex_msg_escaped = extracted_messages.replace('\\\\', '\\\\\\\\').replace('\\'', '\\\\\\'')\n",
    "\n",
    "    content_str = f\"\"\"\n",
    "'### INSTRUCTIONS\n",
    "You are a world-class financial reporter. \n",
    "You will be provided with several summaries from a financial earnings call transcript. \n",
    "Your task is to combine and refine these summaries into a single, coherent article structured in JSON format. \n",
    "\n",
    "The JSON should include a title, section titles, and section bodies. \n",
    "The article should start with an engaging introduction, followed by detailed analysis, the positives, the negatives, and conclude with a summary of the key points.\n",
    "\n",
    "Each section should be comprehensive, detailed, and at least 5-7 sentences in length, providing in-depth insights and analysis.\n",
    "\n",
    "{ex_msg_escaped}\n",
    "\n",
    "### OUTPUT\n",
    "Only provide the output in the following JSON format without any new lines or extra characters \n",
    "    and nothing else, no extra comments JUST THE JSON:\n",
    "{{\n",
    "  \"title\": \"Title of the Article\",\n",
    "  \"sections\": [\n",
    "    {{\n",
    "      \"section_title\": \"Introduction\",\n",
    "      \"section_body\": \"...\"\n",
    "    }},\n",
    "    {{\n",
    "      \"section_title\": \"Detailed Analysis\",\n",
    "      \"section_body\": \"...\"\n",
    "    }},\n",
    "    {{\n",
    "      \"section_title\": \"Positives\",\n",
    "      \"section_body\": \"...\"\n",
    "    }},\n",
    "    {{\n",
    "      \"section_title\": \"Negatives\",\n",
    "      \"section_body\": \"...\"\n",
    "    }},\n",
    "    {{\n",
    "      \"section_title\": \"Conclusion\",\n",
    "      \"section_body\": \"...\"\n",
    "    }}\n",
    "  ]\n",
    "}} \n",
    "\n",
    "### EXCLUDE\n",
    "DO NOT include any new lines or any characters that will invalidate JSON.\n",
    "'\n",
    "\"\"\"\n",
    "    \n",
    "    query = f\"SELECT SNOWFLAKE.CORTEX.COUNT_TOKENS( '{AI_MODEL}', {content_str} ) as TOKEN_LEN;\"\n",
    "    token_len = session.sql(query).to_pandas().iloc[0]['TOKEN_LEN']\n",
    "\n",
    "    if(token_len >= SPLIT_TOKENS-1):\n",
    "        print('----------------------------------------------------------------')\n",
    "        print(f'Token Length Summary Overload: {token_len}')\n",
    "        return \"{}\"\n",
    "        \n",
    "    else:\n",
    "        # Combine and refine results\n",
    "        final_summary_query = f\"\"\"\n",
    "            SELECT SNOWFLAKE.CORTEX.COMPLETE(\n",
    "                '{AI_MODEL}',\n",
    "                ARRAY_CONSTRUCT(\n",
    "                    OBJECT_CONSTRUCT(\n",
    "                        'role', 'user',\n",
    "                        'content', {content_str}\n",
    "                    )\n",
    "                ),\n",
    "                OBJECT_CONSTRUCT('temperature', 0.7)\n",
    "            ) AS RESULT\n",
    "        \"\"\"\n",
    "        \n",
    "        # Execute the final summary query\n",
    "        final_summary = session.sql(final_summary_query).collect()[0]['RESULT']\n",
    "        final_summary = re.sub(r'[\\n\\r\\u0085\\u2028\\u2029]+', ' ', final_summary)\n",
    "        return final_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b96740a-3845-41c7-818b-5e39d89ab2cb",
   "metadata": {
    "collapsed": false,
    "name": "cell7"
   },
   "source": [
    "## Get Additional Company Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2672194a-0d46-435e-872d-fb41741e14ea",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "GetCompanyProfieCell"
   },
   "outputs": [],
   "source": [
    "def getCompanyProfile(ticker):\n",
    "    print('Profile - ', end=\" \")\n",
    "    \n",
    "    def fetch_data(url):\n",
    "        query = f\"SELECT earnings.public.generic_url('{url}') AS DATA\"\n",
    "        q_res = session.sql(query).to_pandas().iloc[0].iloc[0]\n",
    "\n",
    "        def remove_single_quotes_within_double_quotes(match):\n",
    "            return match.group(0).replace(\"'\", \"\")\n",
    "\n",
    "        pattern = r'\"[^\"]*\"'\n",
    "        cleaned_text = re.sub(pattern, remove_single_quotes_within_double_quotes, q_res)\n",
    "\n",
    "        # Check if q_res is an empty list directly\n",
    "        if str(cleaned_text) == '[]':\n",
    "            return None\n",
    "        return cleaned_text.replace('\"', \"'\")\n",
    "\n",
    "    def clean_profile_description(profile):\n",
    "        profile_split = re.split(r'\"', profile)\n",
    "\n",
    "        if len(profile_split) > 1:\n",
    "            # Replace single quotes and remove unwanted characters\n",
    "            new_desc = profile_split[1].replace(\"'\", \"\").replace('\\\\x', '')\n",
    "            return profile_split[0] + \"'\" + new_desc + \"'\" + profile_split[2]\n",
    "        return profile\n",
    "\n",
    "    def parse_profile(profile):\n",
    "        try:\n",
    "            profile_cleaned = clean_profile_description(profile)\n",
    "            profile_json = profile_cleaned.replace(\"'\", '\"').replace('None', 'null').replace('True', '\"true\"').replace('False', '\"false\"')\n",
    "            return json.loads(profile_json)[0]\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f'{ticker} - {e}')\n",
    "            return '{}'\n",
    "\n",
    "    profile_url = f\"https://url/{ticker}\"\n",
    "    profile = fetch_data(profile_url)\n",
    "\n",
    "    # Return empty JSON object if profile is None, otherwise parse the profile\n",
    "    return '{}' if profile is None else parse_profile(profile)\n",
    "\n",
    "# ticker = \"TSLA\"\n",
    "# print(getCompanyProfile(ticker))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3d51b3-d03e-4235-bf5a-74d8f57b77b1",
   "metadata": {
    "collapsed": false,
    "name": "cell8"
   },
   "source": [
    "# Grade Summary\n",
    "Based on the summary we have above, we will ask the LLM to be a world class financial analyst and grade the call for us.\n",
    "\n",
    "The grade will be from ***A+*** to ***F*** based on the investibility of the asset based on the grade of the call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e0a582-e921-4186-983d-e76409e73ce6",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "Grade_summary"
   },
   "outputs": [],
   "source": [
    "def getGrade(summary, ticker):\n",
    "    print(\"Grading - \", end =\" \")\n",
    "    \n",
    "    summary = json.loads(summary)['choices'][0]['messages']\n",
    "    summary_json = extractJSON(summary)\n",
    "\n",
    "    json_string = json.dumps(summary_json)\n",
    "    escaped_json_string = json_string.replace('\\\\', '\\\\\\\\').replace('\"', '\\\\\"').replace(\"'\", \"\\\\'\")\n",
    "\n",
    "\n",
    "\n",
    "    content_str = f\"\"\" \n",
    "'### INSTRUCTIONS\n",
    "You are a world class financial analyst,\n",
    "You will be provided with a summary of a financial call.\n",
    "your task is to grade the call from A+ to F on whether to invest in the asset or not,\n",
    "You must be harsh in your criteria for grading as peoples real funds are at stake. \n",
    "\n",
    "The summary is: {escaped_json_string}\n",
    "\n",
    "### OUTPUT\n",
    "Only provide the output in the following JSON format without any new lines or extra characters \n",
    "    and nothing else, no extra comments JUST THE JSON:\n",
    "{{\n",
    "  \"grade\": \"your grade\",\n",
    "  \"reason\": \"your reasoning for the grade\"\n",
    "}}\n",
    "\n",
    "### EXCLUDE\n",
    "DO NOT include anything that will invalidate proper JSON\n",
    "Do not mention COMPANY ABC or COMPANY XYZ. Use the company ticker or company name if you know it. The company ticker is {ticker}\n",
    "'\n",
    "\"\"\"\n",
    "\n",
    "    \n",
    "    query = f\"SELECT SNOWFLAKE.CORTEX.COUNT_TOKENS( '{AI_MODEL}', {content_str} ) as TOKEN_LEN;\"\n",
    "    token_len = session.sql(query).to_pandas().iloc[0]['TOKEN_LEN']\n",
    "\n",
    "    if(token_len >= SPLIT_TOKENS-1):\n",
    "        print('----------------------------------------------------------------')\n",
    "        print(f'Token Length Summary Overload: {token_len}')\n",
    "        return \"{}\"\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        grade_query = f\"\"\"\n",
    "            SELECT SNOWFLAKE.CORTEX.COMPLETE(\n",
    "                '{AI_MODEL}',\n",
    "                ARRAY_CONSTRUCT(\n",
    "                    OBJECT_CONSTRUCT(\n",
    "                        'role', 'user',\n",
    "                        'content', {content_str}\n",
    "                    )\n",
    "                ),\n",
    "                OBJECT_CONSTRUCT('temperature', 0.7)\n",
    "            ) AS RESULT\n",
    "        \"\"\"\n",
    "        \n",
    "        # Execute the final summary query\n",
    "        finalGrade = json.loads(session.sql(grade_query).collect()[0]['RESULT'])['choices'][0]['messages']\n",
    "    \n",
    "        finalGrade = re.sub(r'[\\n\\r\\u0085\\u2028\\u2029]+', ' ', finalGrade)\n",
    "        grade_json = extractJSON(finalGrade)\n",
    "        return grade_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e760baf-5d1c-4142-955a-f4d70f5895f4",
   "metadata": {
    "collapsed": false,
    "name": "cell9"
   },
   "source": [
    "# Save the Report & the Grade to Snowflake\n",
    "Give all the information to this function to save the data to ***EARNINGS_SUMMARIES*** table in Snowflake.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71017ef2-c800-4f39-b706-b3e64ff3713d",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "saveReportCell"
   },
   "outputs": [],
   "source": [
    "def saveReport (final_summary, ticker, quarter, year, release, grade, earnings_actual, profile, date):  \n",
    "    print('saving - ', end =\" \")\n",
    "    summary = re.sub(r'[\\n\\r\\u0085\\u2028\\u2029]+', ' ', final_summary)\n",
    "    summary = json.loads(final_summary)['choices'][0]['messages']\n",
    "    summary_json = extractJSON(summary)\n",
    "\n",
    "    # Extract the title and summary JSON\n",
    "    title = summary_json[\"title\"]\n",
    "    sections = summary_json['sections']\n",
    "    \n",
    "    # Create a DataFrame for insertion\n",
    "    data = {\n",
    "        \"STOCK_SYMBOL\": [ticker],\n",
    "        \"TITLE\": [title],\n",
    "        \"FISCAL_QUARTER\": [quarter ],\n",
    "        \"FISCAL_YEAR\": [ year ],\n",
    "        \"RELEASE_DATE\": [ release ],\n",
    "        \"QUERY_DATE\": [ date ],\n",
    "        \"GRADE\": [ json.dumps(grade) ],\n",
    "        \"EARNINGS\": [ json.dumps(earnings_actual) ],\n",
    "        \"PROFILE\": [ json.dumps(profile) ],\n",
    "        \"SUMMARY_JSON\": [json.dumps(sections)]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    session.use_database(\"EARNINGS\")\n",
    "    session.use_schema(\"ANALYSIS\")\n",
    "    # Write the DataFrame to the Snowflake table using the fully qualified table name\n",
    "    return session.write_pandas(df, \"EARNINGS_SUMMARIES\", auto_create_table=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961c189f-944e-4e37-96ff-640c9103b609",
   "metadata": {
    "collapsed": false,
    "name": "cell10"
   },
   "source": [
    "# Run Everything\n",
    "Use all the functions above to run everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7761ed-2641-4223-8c0e-ff2808606185",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "GradeAllTranscripts"
   },
   "outputs": [],
   "source": [
    "# transcripts = GetTranscriptsToProcess.to_pandas()\n",
    "transcripts = TranscriptsToProcess.to_pandas()\n",
    "\n",
    "for index, row in transcripts.iterrows():\n",
    "    try:\n",
    "        ticker = row['STOCK_SYMBOL']\n",
    "        transcript = row['TRANSCRIPT']\n",
    "        q = row['FISCAL_QUARTER']\n",
    "        y = row['FISCAL_YEAR']\n",
    "        r = row['RELEASE_DATE']\n",
    "        qd = row['QUERY_DATE']\n",
    "        ea = ''\n",
    "    \n",
    "        print(f'{index} - {ticker} - {y} - {q}', end =\" \")\n",
    "        summary = splitTransacript(transcript)\n",
    "        report = summarizeTransacript(summary)\n",
    "        profile = getCompanyProfile(ticker)\n",
    "        grade = getGrade(report, ticker)\n",
    "        print(grade)\n",
    "        \n",
    "        res = saveReport(report, ticker, q, y, r, grade, ea, profile, qd)\n",
    "        \n",
    "        print(f' {res}')\n",
    "    \n",
    "    except TypeError as e:\n",
    "        print(f\"TypeError for ticker {ticker}: {e}\") \n",
    "    except Exception as e:\n",
    "        print(f\"Exception for ticker {ticker}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
